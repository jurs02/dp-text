\chapter{Creation of sentiment lexicon}
Through some initial experimentation with various existing sentiment lexicons such as the SenticNet sentiment lexicon and the NLTK's SentiWordNet sentiment lexicon it was discovered that these universal sentiment lexicons are not well suited for application on the domain of conference paper reviews.

One issue is that both of these sentiment lexicons assign sentiment polarity on a scale. Therefore most dictionary words have a certain sentiment polarity which was considered inappropriate in this task, as words which would generally be considered neutral should not somehow skew the polarity of words that might actually be important. These words often have a polarity around the center of the polarity interval (for example if the polarity is assigned on a scale from -1 to 1 they usually have polarity somewhere around 0) and could be removed by using some polarity threshold. However, that might also lead to losing some words that are actually important for sentiment classification in this domain. For example in SenticNet, the word \textit{clarification} has polarity of -0.09, but it is often used in sentences such as \textit{``The section about experimental results needs some clarification''} where the sentiment is clearly negative.

Another possible issue with pre-made sentiment lexicons is that they mostly do not include punctuation. However, punctuation might have great semantic significance in conference paper reviews, as they are often written in plaintext and punctuation is used to compensate for the lack of usual formatting styles such as bullet points. 


For that reason, it was decided to create a custom, domain-specific sentiment lexicon. This chapter describes the process of compiling a sentiment lexicon from a set of annotated sentences taken from reviews using the Na\"ive Bayes classifier.
\section{Implementation of sentiment lexicon generation using Naive Bayes}
The Na\"ive Bayes classifier, as described in section \ref{sec:NBC} is a probabilistic classifier which needs a training dataset of labeled data in order to determine the influence of different evidence on the class. The obtainment of this dataset is described in section \ref{sec:data_sl}.

NLTK's implementation of the classifier was used. Of all the lemmatized tokens in the reviews, the number of tokens the classifier needs to process was limited to 450. The tokens are considered features by the algorithm, each token being transformed into a column where the value of the column for each row representing a review is true or false depending on the presence of the token in the review. This was achieved thanks to the \texttt{FreqDist} class from the NLTK's \texttt{probability} module.

The dataset of labeled review sentences was split to have a testing dataset of 50 example sentences to determine the accuracy of the classifier and the rest of sentences was used for training. The test dataset is fairly small, but it was considered far more preferable to leave most examples to the training dataset to produce a hopefully more accurate sentiment lexicon, even though the trade-off is not being able to judge the created lexicon in great detail in this phase. 
That being said, the accuracy of the classifier on the testing dataset was 0.78, meaning 78~\% of sentences were classified correctly. 

The classifier allows us to get a list of features which have the highest contribution to classification through its \texttt{show\_most\_informative\_features} method which based on the number we specify as its argument outputs a list of features with their ratio of occurrences in negative and positive sentences. The output when applied to the training data can be seen in Table \ref{tab:nbc_output}. It shows that it is more than 35 times more likely that the word \textit{easy} occurs in a sentence labeled as \textit{positive} while a question mark occurs far more often in negative sentences.

\begin{table}[htbp!]
\caption{Output of the Na\"ive Bayes classifier}\label{tab:nbc_output}
\centering
\begin{tabular}{lll}
\textbf{Most Informative Features} & \textbf{}         & \textbf{}  \\
easy = True                        & positi : negati = & 35.8 : 1.0 \\
interesting = True                 & positi : negati = & 15.3 : 1.0 \\
but = True                         & negati : positi = & 14.8 : 1.0 \\
topic = True                       & positi : negati = & 13.7 : 1.0 \\
? = True                           & negati : positi = & 12.3 : 1.0 \\
what = True                        & negati : positi = & 10.9 : 1.0 \\
sound = True                       & positi : negati = & 10.6 : 1.0 \\
community = True                   & positi : negati = & 9.9 : 1.0  \\
not = True                         & negati : positi = & 8.9 : 1.0  \\
idea = True                        & positi : negati = & 8.1 : 1.0  \\
interest = True                    & positi : negati = & 7.9 : 1.0  \\
good = True                        & positi : negati = & 7.5 : 1.0  \\
clearly = True                     & positi : negati = & 7.4 : 1.0  \\
well = True                        & positi : negati = & 7.4 : 1.0  \\
me = True                          & negati : positi = & 7.2 : 1.0  \\
bring = True                       & positi : negati = & 6.8 : 1.0  \\
valuable = True                    & positi : negati = & 6.4 : 1.0  \\
why = True                         & negati : positi = & 5.6 : 1.0  \\
conference = True                  & positi : negati = & 5.6 : 1.0  \\
write = True                       & positi : negati = & 5.5 : 1.0  \\
effort = True                      & positi : negati = & 5.4 : 1.0  \\
highly = True                      & positi : negati = & 5.2 : 1.0 
\end{tabular}
\end{table}
The \texttt{show\_most\_informative\_features} method was then adjusted to create a function which transforms these ratios of occurrences in sentences with positive or negative sentiments into a sentiment lexicon. 

Each of the most informative tokens is given a value of -1 or +1 depending on if they occur more often in negative or positive sentences (where -1 corresponds to negative sentiment and  +1 corresponds to positive sentiment). 

\section{Created sentiment lexicon}
From the list of most informative features, the top 100 words were chosen to be included to the sentiment lexicon (setting the ratio threshold at 2.4 : 1.0). 
Then the results were compared with the SenticNet sentiment lexicon, to see what is the level of agreement between the two lexicons and it was discovered that in 13 cases, the polarity of the sentiment of words found in both lexicons differed and in 31 cases a word from my lexicon was not found in SenticNet. Surprisingly not all the words that were not found in SenticNet were not found due to the aforementioned lack of punctuation in SenticNet or because these words could truly be considered neutral. SenticNet was missing some words which are considered fairly meaningful in sentiment analysis such as \textit{rather} or \textit{should}.

A list of 40 positive and 66 negative words compiled manually during the process of labeling the training dataset was also included. The resulting sentiment lexicon contains 186 sentiment words out of which 88 have positive polarity of +1 and 98 have a negative polarity of -1.



