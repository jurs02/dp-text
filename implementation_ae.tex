\chapter{Implementation of aspect extraction}
\label{sec:ae}
In order to create a lexicon of terms that represent the chosen set of criteria, to be used for identifying aspect expressions in the reviews, it was decided to use two main approaches. One is the taxonomy extraction mentioned in \ref{sec:taxonomy} and the second one is extraction of frequent words used by the reviewers for different criteria in a text that is already divided by headers into sections for the respective criterion.
\section{Manually created taxonomy}
As described in section \ref{sec:taxonomy}, in order to perform taxonomy extraction, first a user defined taxonomy needs to be created.
The original idea is that the taxonomy is a hierarchical representation where the top level of the hierarchy represents the feature of an object and the following levels represent the aspects of that feature. Since in this case, it was not necessary to separate the chosen criteria any further this representation was used to have the main criteria in the top level and only have one level underneath the top level, to specify possible aspect expressions for each criterion. These terms serve as a seed for future expansion of the taxonomy by including extracted crude features with enough similarity to these expressions. 

You can see this taxonomy in Listing \ref{lst:json_tax_m}.

\begin{lstlisting}[language=json,firstnumber=1, caption={Manually created taxonomy for aspect extraction},label={lst:json_tax_m},float,floatplacement=H]
{
    'relevance': {
        'appropriateness', relevance'
    },
    'novelty': {
        'originality', 'innovativeness', 'innovation', 
        'novelty of contribution', 'novelty', 'impact', 
        'significance'
    },
    'technical quality': {
        'scientific quality', 'implementation', 'soundness',
        'technical quality'
    },
    'state of the art': {
        'scholarship', 'references', 'related work', 
        'state of the art'
    },
    'evaluation': {
    	'reproducibility', 'evaluation', 'evaluate', 'evaluating'
    },
    'presentation': {
    	'clarity', 'quality of writing', 'presentation',
    	'typo', 'description', 'describe', 'written'
    },
}
\end{lstlisting}

\section{Crude features extraction}
The next step of taxonomy based extraction is to obtain a set of crude features. The next two subsections describe the extraction algorithms inspired by the frequency-based method of aspect extraction (see section \ref{sec:freq_extr_t}).

\subsection{Extraction of frequent nouns and noun phrases}
The first method of extracting terms that are likely to represent a criterion is to extract frequent nouns and noun phrases (NP). For that the content of the file (representing a single review) is tokenized and each token is assigned a Part-of-Speech (POS) tag. Then, to get the nouns and noun phrases, the extracted tuples (token, pos\_tag) are parsed to determine the multi-token sequences which represent nouns and noun phrases. 

The RegexpParser from the NLTK library was used for the parsing, which utilizes a user defined grammar, consisting of labeled regular expression rules, describing the sequence of POS tags we want to assign the label to. The result of the parsing is a tree structure, where each sequence corresponding to the regular expression is labeled accordingly, so this allows us to pick the subtrees labeled by the parser as noun phrases. 

The code snippet which shows the defined grammar for NP extraction can be seen in Listing~\ref{lst:np_grammar}. 
The grammar uses POS tags, so NN are nouns, IN are prepositions and JJ are adjectives. The first regular expression, labeled NBAR, searches for nouns and adjectives, terminated with nouns, allowing us to discover phrases like \textit{black box}, where the meaning changes when we consider both of these words separately. The second regular expression, labeled NP, looks for NBAR expressions connected with prepositions such as \textit{of}, \textit{in} etc. \cite{nltk_np}

\begin{lstlisting}[language=python,caption={Grammar for the extraction of noun phrases.}, label={lst:np_grammar}, xleftmargin=.2\textwidth, xrightmargin=.2\textwidth],float,floatplacement=H]
RegexpParser("""
        NBAR:
            {<NN.*|JJ>*<NN.*>} 

        NP:
            {<NBAR>}
            {<NBAR><IN><NBAR>}
        """)
\end{lstlisting}

In order to then extract only the wanted noun phrase sequences, the tree is traversed. For each subtree, labeled as NP,  each token of the sequence is lemmatized, it is checked for if its length is at least two characters, but less than 20 characters, and if so, the lemmatized tokens are joined into a single string and appended to the list of NPs in the file.

By performing this extraction with each file, the result is a list of lists, where each list represents the extracted nouns and noun phrases from one file. To then obtain the ones that are frequent enough across all the reviews, and may therefore represent the criteria, the support of a noun phrase across the reviews is calculated. The following equation represents the calculation of the support metric for a word $w_{i}$ where $N_{w_{i}}$ is the number of reviews containing the word $w_{i}$ and $N$ is the total number of reviews:

\begin{equation}
support(w_{i})=\frac{N_{w_{i}}}{N}
\end{equation}

In order to perform this calculation the data is transformed into a matrix, where each row represents one review, each column represents a criterion expression candidate. The values of an element in \textit{row-i} and \textit{column-j} is 1 if the criterion expression candidate \textit{j} is present in review \textit{i} or 0 if it is not. To get the support of a criterion expression candidate \textit{j}, the sum of \textit{column-j} is divided by the total number of rows. If the support is greater than the minimum support, the criterion expression candidate is kept, if not, it is discarded. Through various experiments it was found that the best value for minimum support is 2~\%, which gives us reasonable candidates for criterion expressions, but does not include too many candidates to make the process of manually confirming them too tedious. It may seem like a very ``minimal'' minimal support, however the number of reviews that were at disposal to extract frequent noun phrases was fairly small, so a small support was necessary to account for that fact. It should not be too big of an issue though, considering that the candidates are further tested to determine their similarity to the manually created taxonomy, which reduces the number of criterion expression candidates the user has to go through.

\subsection{Extraction of frequent adjectives}
Although the original frequency-based algorithm (see section \ref{sec:freq_extr_t}) only focuses on nouns and noun phrases, by going through the training data, it was discovered that fairly often, the criterion expression found in the reviews take the form of adjectives. 

Consider the phrase \textit{``The topic addressed by the paper relevant to the conference.''}. Here, the adjective \textit{relevant} evidently corresponds to the criterion \textit{relevance}. Because of that, it was decided to extract frequent adjectives from the reviews as well and again calculate the support of each adjective across the reviews with the same technique as was used with the noun phrase extraction. 
\section{Extraction by review structure}
The data from the 2018 ESWC conference that was obtained had the review text divided into sections where the different sections represented the different criteria. The structure of these reviews can be seen in Figure \ref{img:eswc_2018}, where in bold are the structural part of the review format which stays the same across all reviews. 

It was decided to leverage this data to extract frequent words from each one these sections across the reviews. 
The frequent words of each section are included in the new aspect expression taxonomy directly, they do not go through the same process of similarity matching against the manually created taxonomy as the candidates that were chosen purely on their frequency. This is useful because it allows to extract new possible criterion terms that would not match with any of the terms in the taxonomy, which were originally not thought to be included.

Each term frequent enough in a criterion section across all the reviews is included in the taxonomy based on a match between the ESWC set of criteria and the chosen set of metrics. The mapping between the two sets of metrics was done according to Table \ref{table:conf}.

The aspect expression candidates created by this method are still evaluated by the user as explained in section \ref{sec:user_val}.
\begin{figure}[htb]
        \centering
        \fbox{\begin{minipage}{0.9\textwidth}

\textbf{Relevance to ESWC: } 

numerical score (score interpretation)

\textbf{Novelty of the Proposed Solution: } 

numerical score (score interpretation)

\textbf{Correctness and Completeness of the Proposed Solution: }

numerical score (score interpretation)

\textbf{Evaluation of the State-of-the-Art: }

numerical score (score interpretation)

\textbf{Demonstration and Discussion of the Properties of the
 Proposed Approach: }
 
numerical score (score interpretation)
 

\textbf{Reproducibility and Generality of the Experimental Study: }

numerical score (score interpretation)

\textbf{Overall score: }

numerical score (score interpretation)

\textbf{Reviewer's confidence: }

numerical score (score interpretation)

\textbf{Open Reviewing Opting Out:  }

numerical score (score interpretation)

\textbf{Overall evaluation (*Resources and In-Use tracks only*, Research reviewers please only put "n/a"): }

numerical score (score interpretation)





\textbf{----------- Relevance to ESWC ----------- }

reviewer's comment

\textbf{----------- Novelty of the Proposed Solution ----------- }

reviewer's comment

\textbf{----------- Correctness and Completeness of the Proposed Solution ----------- }

reviewer's comment

\textbf{----------- Evaluation of the State-of-the-Art -----------}

reviewer's comment

\textbf{----------- Demonstration and Discussion of the Properties of the Proposed Approach ----------- }

reviewer's comment

\textbf{----------- Reproducibility and Generality of the Experimental Study ----------- }

reviewer's comment

\textbf{----------- Overall score -----------}

reviewer's comment

\end{minipage}}
\caption{ESWC 2018 review structure}
\label{img:eswc_2018}
\end{figure}

\section{Similarity matching against the manually created taxonomy}
After a set of crude features is obtained, the next step is to map these features to the user defined taxonomy. As previously described, the similarity of a crude feature to the aspect expression in the taxonomy needs to be calculated. If the feature is similar enough, it is passed to the final step of the taxonomy extraction, which is an interactive revision process. This process is described in more detail in the next section.

For measuring the similarity between two terms, it was determined to use the WordNet tool and its \texttt{path\_similarity} metric which returns a score denoting how similar two word senses are, based on the shortest path that connects the senses in the hypernym hierarchy. The score is in the range 0 to 1 where 1 denotes identity (when word is compared to itself). \cite{nltk_ps}

The main obstacle of using this metric is that it does not work well with adjectives. That is because all nouns are part of one big hierarchy, but that is not the case for other parts of speech such as an adjective, an adverb etc. So for example the similarity for the words ``relevance'' and ``relevant'' is zero, even though the words are closely related. Because the extracted crude features may contain adjectives (in the case of noun phrases) or be adjectives themselves (in the case of frequent adjectives extraction),  a workaround was implemented by transforming the adjectives to their closest related noun, and perform the similarity measurement on these nouns. If the similarity of these nouns is greater than the similarity threshold, the original (albeit lemmatized) word is passed on. 

Another issue is measuring similarity with terms consisting of multiple words. Certain multi-token terms are already present in the WordNet thesaurus (such as \textit{state of the art}), and getting their synsets to perform similarity matching is as simple as replacing the spaces between words with underscores. However, some multi-token words included both in the user defined taxonomy and in the crude features cannot be found in WordNet directly.
It was solved by calculating the maximum similarity between each token of one term to all the tokens of the other term. Of these similarities the maximal one is chosen.

The final pseudocode for similarity measuring between two terms is in Figure \ref{img:sim_alg}. When comparing frequent adjectives to the taxonomy, the part-of-speech argument is set accordingly.
\begin{figure}[!htb]

\begin{algorithm}[H]
\KwData{term 1, term 2, part-of-speech}
\KwResult{the similarity of the term as a numeric score between 0 and 1}
\For{both terms}
   {	
	\If {the term is just one word}
		{
		term\_synsets = find all synsets of the term \;
		\If{the part-of-speech is adjectives}
			{term\_synsets += find all synsets of the closest related noun}
	}

	\Else
		{ 
		term\_underscored = substitute all spaces in the term with underscores \;
		term\_synsets = find all synsets of term\_underscored \;
	}
	\If{no synsets are found for a multi-word term}
		{
		term\_synsets = synsets of all words in the term (including transformed adjectives) \;
		}
}
score = maximum similarity between all term\_synsets of term1 and term\_synsets of term2 \;
\Return{score}

\end{algorithm}
 \caption{Algorithm for similarity measurement between two terms}
	\label{img:sim_alg}
\end{figure}


Another issue of wordnet's \texttt{path\_similarity} is that it is asymmetrical. So sometimes \texttt{path\_similarity(x,y)} returns \texttt{None} or 0 while \texttt{path\_similarity(y,x)} returns a non-zero value. This is because for some words, a fake root in the hierarchy may be added to find a path between to two words, but this depends on the order in which the two words are supplied to the \texttt{path\_similarity} function. 
Therefore the final calculation of similarity between two terms term1 and term2 is defined as the maximum between similarity(term1,term2) and similarity(term2,term1).

The threshold for similarity of a term to the taxonomy was set to 0.3. Therefore every term with a similarity to any term in the manually created taxonomy equal or greater than the threshold will become a criterion expression candidate under the same criterion as the term it was most similar to.
\section{User validation of final taxonomy}
\label{sec:user_val}
When aspect expression candidates are generated and sorted by the criterion they most likely represent they have to pass the final validation. This validation is a manual process, where a user goes through every criterion expression candidate and decides between three options:
\begin{itemize}
\item The criterion expression candidate is added under the criterion which was algorithmically determined as the most probable.
\item The user disagrees with the most probable criterion  and sorts the criterion expression candidate under a different criterion.
\item The user decides not to include the criterion expression candidate in the taxonomy at all.
\end{itemize}

The interactive process of the user validation of the final taxonomy can be seen in Figure \ref{img:user_tax}.
\begin{figure}[!htb]

\begin{commandshell}
Does the term "topic" belong under aspect "relevance" ? [y/n]
@y@
Does the term "relation" belong under aspect "relevance" ? [y/n]
@n@
Does it belong under any of these aspects ?:
relevance                [a]
novelty                  [b]
technical quality        [c]
state of the art         [d]
evaluation               [e]
presentation             [f]

none of the above        [n]
@n@
Does the term "introduction" belong under aspect "novelty" ? [y/n]
@n@
Does it belong under any of these aspects ?:
relevance                [a]
novelty                  [b]
technical quality        [c]
state of the art         [d]
evaluation               [e]
presentation             [f]

none of the above        [n]
@p@
\end{commandshell}
\caption{The interactive process of user validation of proposed aspect taxonomy.}
\label{img:user_tax}
\end{figure}
\section{Resulting taxonomy of aspects}
The entire process of aspect expressions extraction and user validation (carried out by the thesis' author) resulted in 57 new terms in the taxonomy. The new terms added under each criterion are showcased in Table \ref{tab:ae_results} along the original terms from the manually created taxonomy.
\begin{table}[!htbp]
\caption{Result of the aspect expression extraction}
\label{tab:ae_results}
\begin{tabular}{l|ll}
\textbf{criterion} & \textbf{aspect expressions - old}                                                                                                               & \textbf{aspect expressions - new}                                                                                                                                                                                                                                              \\ \hline
relevance          & appropriateness, relevance                                                                                                                      & \begin{tabular}[c]{@{}l@{}}important topic, relevant, \\ contribution, topic\end{tabular}                                                                                                                                                                                      \\ \hline
novelty            & \begin{tabular}[c]{@{}l@{}}originality, innovativeness,\\ innovation, novelty, \\ novelty of contribution, \\ impact, significance\end{tabular} & \begin{tabular}[c]{@{}l@{}}originality innovativeness,\\ scientific contribution,\\ improvement, novel, idea\end{tabular}                                                                                                                                                      \\ \hline
technical quality  & \begin{tabular}[c]{@{}l@{}}scientific quality, \\ implementation, soundness, \\ technical quality\end{tabular}                                  & \begin{tabular}[c]{@{}l@{}}running example,  scalability,  code, \\ design, usability,\\ implementation and soundness, \\ technical detail\end{tabular}                                                                                                                        \\ \hline
state of the art   & \begin{tabular}[c]{@{}l@{}}scholarship, references,  \\ related work, \\ state of the ar\end{tabular}                                           & \begin{tabular}[c]{@{}l@{}}reference, \\ related work section,\\ references, benchmark, comparison, \\ previous work, related research\end{tabular}                                                                                                                            \\ \hline
evaluation         & \begin{tabular}[c]{@{}l@{}}reproducibility,  evaluation, \\ evaluating, evaluate\end{tabular}                                                   & \begin{tabular}[c]{@{}l@{}}evaluation section, coverage, \\ experimentation, score,\\ experimental result, experimental, \\ experimental evaluation, support,  \\ empirical evaluation, accuracy, \\ assessment, evaluation result, \\ recall, metric, experiment\end{tabular} \\ \hline
presentation       & \begin{tabular}[c]{@{}l@{}}clarity, quality of writing, \\ presentation, typo,\\ written, describe,\\ description\end{tabular}                  & \begin{tabular}[c]{@{}l@{}}english, scientific paper, notation, text,\\ last sentence, sec, write, figure, \\ introduction, document, explained, \\ current form, reading, writing, \\ first paragraph, intro, readability, \\ format, paragraph\end{tabular}                 
\end{tabular}
\end{table}