\chapter{Implementation of aspect extraction}
\section{Aspect extraction}
In order to create a lexicon of terms that represent the chosen set of criteria, to be used for identifying aspect expressions in the reviews, I have decided to use two main approaches. One is the taxonomy extraction mentioned in INSERT and the second one is extraction of frequent words used by the reviewers for different criteria in a text that's already divided by headers into sections for the respective criterion.
\subsection{Manually created taxonomy}
As described in section INSERT, in order to perform taxonomy extraction, we need to manually
create a user defined taxonomy. 

The original idea is that the taxonomy is a hierarchial representation where the top level of the hierarchy represents the feature of an object and the following levels represent the aspects of that feature. Because in my case, I don't need to seperate the chosen criteria any further I have used this representation to have the main criteria in the top level and only have one level underneath the top level, to specify possible aspect expressions for each criterion, which serves as a seed for future expansion of the taxonomy by including extracted crude features with enough similarity to the these expressions. 

You can see this taxonomy on image INSERT.
\subsection{Crude Features extraction}
The next step of taxonomy based extraction is to obtain a set of crude features. The next two subsections describe the extraction algorithms based on Bings method of aspect extraction.

\subsubsection{Extraction of frequent nouns and noun phrases}
The first method of extracting terms that are likely to represent an aspect is to extract frequent nouns and noun phrases. For that the content of the file is tokenized and each token is assigned a Part-of-Speech tag. Then, to get the nouns and noun phrases, the extracted tuples (token, pos\_tag) are parsed to determine the multi-token sequences which represent nouns and NNPs. For the parsing I've used the RegexpParser, which utilizes a user defined grammer, consisting of labeled regular exptression rules, descibing the sequence of POS tags we want to asign the label to. The code snippet shows the deffined grammer for NP extraction and the result of the parsing which is a tree, can be seen on INSERT.

In order to then extract only the wanted noun phrase sequences, I traverse the tree, and for each subtree, labeled as NP, I lemmatize each token of the sequence, check if it's length is at least two characters, but less than 20 characters, and if so, I join the lemmatized tokens into a single string and append it to the list of NPs in the file.

By performing this extraction with each file, I get a list of lists, where each list represents the extracted nouns and noun phrases from one file. To then obtain the ones that are frequent enough across all the reviews, and may therefore represent aspects, I calculate the support of a noun phrase across the reviews. The equation INSERT represent the calculation of the support metric. In order to perform this calculation I transform the data into a matrix, where each row represents one review, each column represents an aspect candidate and the values of an element in row-i and column-j is 1 if the aspect candidate j is present in review i or 0 if it is not. To get the support of an aspect candidate j, I than divide the sum of column-j by the total number of rows. If the support is greater than the minimum support, the aspect candidate is kept, if not, it's discarded. 

\subsubsection{Extraction of frequent adjectives}
Although Bing only focuses on nouns and noun phrases, by going through the training data, I have discovered that fairly often, the aspect expression found in the reviews take the form of adjectives. 

Consider the phrase ``The topic addressed by the paper relevant to the conference.''. Here, the adjective \textit{relevant} evidently corresponds to the aspect \textit{relevance}. Because of that, I have decided to extract frequent adjectives from the reviews as well and again calculate the support of each adjective across the reviews with the same technique as I have used with the noun phrase extraction. 
\subsection{Extraction by review structure}
The data from the 2018 ESWC conference I've obtained had the review text divided into sections where some of the sections represented the different criterias. I have decided to leverage this data to extract frequent words from each one these sections accross the reviews. This is usefull because it allows to extract new possible aspect terms that would not match with any of the terms in the taxonomy, because I originally did not think to  include them.
\subsection{Similarity matching againts the manually created taxonomy}
After we obtain a set of crude features, the next step is to map these features to the user defined taxonomy. As previously described, we need to calculate the similarity of a crude feature to the aspects in the taxonomy. If the feature is similar enough, it is passed to the final step of the taxonomy extraction, which is an interactive revision process. This process is described in more detail in the next section.

For measuring the similarity between two terms, I've decided to use the wordnet tool and it's path similarity metric. The main obstacle of using this metric is that it doesn't work well with adjectives. Because the extracted crude features may contain adjectives (in the case of noun phrases) or be adjectives themselves (in the case of frequent adjectives extraction), I have decided to implement a workaround by transforming the adjectives to their closest related noun, and perform the similarity measurement on these nouns. If the similarity of these nouns is greater than the similarity threshold, the original (albeit lemmatized) word is passed on.

Another issue is measuring similarity with terms consisting of multiple words. Certain multi-token terms are already present in the wordnet theausaur (such as 'state of the art'), and getting their synsets to perform similarity matching is as simple as replacing the spaces between words with underscores. However some multi-token words included both in the user defined taxonomy and in the crude features cannot be found in wordnet directly. To solve this issue I've decided to calculate the maximum similarity between each token of one term to all the tokens of the other term and define the final similarity as the average similarity of these result, as can be seen in equation INSERT.

\subsection{User validation of final taxonomy}
\subsection{Resulting taxonomy of aspects}
